\documentclass[twoside]{article}
\usepackage[english]{babel}
\usepackage{seminarpaper}
\usepackage[backend=bibtex,style=numeric]{biblatex}
\usepackage{csquotes}
\usepackage{tikz}


\usetikzlibrary{positioning}
\usetikzlibrary{matrix}

\addbibresource{references.bib}

% TODO: temporary for having the "for the reader" section start from 0
\setcounter{section}{-1}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\papertitle{Transformers in Language Models}

\paperauthor{Mikael Fredriksson}

\paperaddress{ } ]

\begin{abstract}
  This paper discusses three language models based on the transformer architecture.
  I aim decipher some differences between the models which seem very similar at first glance.
  The paper also takes a look at some applications and extensions to basic language
  models and some problems present in large language models, including biases
  and hallucination.
\end{abstract}

\section{For the reader}
There's a bit of length left in this paper for some extra explanations,
examples and images, so if there's anything specific you would like to
read about, please indicate that in your review. It can be quite hard to 
gauge the level of difficulty of different topics after being submerged 
in them for a while. Please keep in mind that I should not overlap too much
with specific language model applications nor the general transformer
architecture, since those are their own distinct topics assigned to 
other people. 

\section{Introduction}
This paper will take a look at three popular language models utilizing
transformers in their architecture. It will also give a quick overview
of some applications and additions to the generic model. The aim is to
give the reader a general overview of the current landscape of different
transformer based language model architectures and some example applications.

The three different models are all similar in architecture being mostly
faithful to the original transformer architecture as presented in 
\cite{vaswani_attention_2017}. However, each of them have a different approach
to using the decoder and encoder stacks of the transformer. GPT uses only
the decoder, BERT only the encoder and T5 both encoder and decoder
\cite{radford_improving_nodate,devlin_bert_2019,raffel_exploring_2020}.

\section{Training}
All three of the models this paper discusses use transfer learning for 
training. The general idea is to first train a baseline model unsupervised 
using a large amount of unstructured text and then fine tuning the model
for each specific task after. 

The objective of the unsupervised pre-training stage is to teach the model
general language understanding. The exact details between models varies but
in general, a massive number tokens are fed into to model during the course
of training. Each word corresponds to a single or a few tokens depending on 
the encoding used [citation needed, same as pricing] and the number of tokens
used in training can go up to the trillions \cite{raffel_exploring_2020}. 
The model is asked to predict a missing token based on context. 

In the fine-tuning stage, the pre-trained model is used as a baseline. A new
model is trained using supervised learning with a task specific data set. 
The training can fine-tune all the parameters of the model, as in BERT 
\cite{devlin_bert_2019}, or an additional layer can be added and then
trained, as in GPT \cite{radford_improving_nodate}.

\subsection{Attention masking}
Attention masking prevents the model from attending to some tokens in the input. At any
given time $j$, the model is allowed to see input up to index $i$. Masking is used to prevent the model from
attending to future input which would trivialize training. \cite{raffel_exploring_2020}

The original transformer's encoder-decoder architecture as specified in 
\cite{vaswani_attention_2017} corresponds to the leftmost masking pattern as
specified in figure \ref{masking_patterns}. T5 uses this architecture and masking pattern
\cite{raffel_exploring_2020}. As previously mentioned, BERT uses an encoder
only architecture \cite{devlin_bert_2019} and uses a fully-visible masking 
pattern with a special classification token \cite{raffel_exploring_2020}. GPT on
the other hand, uses a decoder only architecture \cite{radford_improving_nodate}
and uses causal masking \cite{liu_generating_2018}.

Prefix masking can be useful for example when translating. The prefix allows the
model to attend to the entire source language sentence and causal masking can be 
used only for the target language sentence prediction. \cite{raffel_exploring_2020}

Calculating the entire attention history for a long sequence can be an expensive operation.
It is possible to use a sparse version of the attention masking pattern and still achieve 
comparable performance to a full attention mask. A sparse transformer with a fixed attention
pattern includes cells that can summarize the previous locations and propagate 
that forward. This allows computing longer sequences of input tokens and is used in GPT-3
\cite{brown_language_2020}. An illustrating example of different masking patterns
can be seen in figure \ref{sparse_transformer}. \cite{child_generating_2019}

\begin{figure}[h]
  \centering
  \includegraphics*[scale=0.15]{img/attention_masking.png}
  \caption{
    Different attention masking patterns seen in transformer based language models.
    Dark squares indicate that the attention mechanism can see the input $i$ at time $j$.
    Light squares indicate that the access is blocked for a given pair $i$, $j$. 
    \cite{raffel_exploring_2020}
  }
\end{figure}
\begin{figure}[h]
  \centering
  \includegraphics*[scale=0.15]{img/lm_masking.png}
  \caption{
    Different masking patterns used within language models. Dark lines correspond to 
    fully-visible masking. Light gray lines indicate causal masking.
    \cite{raffel_exploring_2020}
  }
\end{figure}
\label{masking_patterns}

\begin{figure}[h]
  \centering
  \includegraphics*[scale=0.1]{img/sparse_transformer.png}
  \caption{
    Sparse Transformer. Visualizing the attention masking pattern on a 6x6 image
    for two attention heads. On the left, the original transformer. In the middle,
    a sparse transformer using striding and on the right, a fixed sparse transformer.
    \cite{child_generating_2019}
  }
\label{sparse_transformer}
\end{figure}

\subsection{Vocabulary}
Different model architectures use different vocabularies. These are chosen based
on the differing requirements of the models. The vocabulary needs to accommodate 
multiple languages and proper nouns while using as few tokens as possible per word.
Memory usage also has to be balanced and the number of tokens cannot grow infinitely
large. 

\section{Models}

The models this paper discusses closely follow the initial architecture
presented in the paper originally proposing the transformer 
\cite{vaswani_attention_2017}. Most of the differences between 
the models come down to training, attention layer masking and the encoder-decoder layers
\cite{raffel_exploring_2020}.


\subsection{Generative Pre-Trained Transformer}
Generative Pre-Trained Transformer, more commonly known as GPT, is a model
which is focused mostly on text generation \cite{brown_language_2020}. 
GPT uses only the decoder of the transformer with 12 layers 
\cite{radford_improving_nodate}. The training is split into unsupervised
pre-training and supervised fine-tuning. 

In the unsupervised pre-training phase, the model is given a set of long form text. 
The supervised fine-tuning is used to train the
model for a specific task. In the supervised fine-tuning phase, an additional linear
output layer is added
and the pre-trained model's activation layer and linear output layer are trained
to the specific task. \cite{radford_improving_nodate}

As the number of parameters in the model grows past a certain threshold, it
is no longer necessary to fine-tune the model for each specific use case 
\cite{brown_language_2020}. Depending on the task at hand, it might be sufficient
to just give the model some examples of the task initially and it will (understand what you want it to do). The examples are given to
the model after training during inference and thus differs from fine-tuning. 
This is called few-shot learning. Special cases of this are one-shot learning which is
otherwise similar to few-shot learning except the number of examples is one.
Zero-shot learning refers to giving the task description using natural language,
kind of like you were explaining it to a human. 

As an example \cite{brown_language_2020} used English to French translation.
The researchers gave the model between 10 to 100 pairs of examples of the
same sentence in English and French. The last pair had just an English sentence
and the model was expected to produce the corresponding sentence translated into
French.

While few-shot learning requires less data and computing power, which makes it more
accessible compared to fine-tuning a large model, at least during the writing of 
\cite{brown_language_2020} failed to produce comparable results to fine-tuning.
The number of examples given is restricted by the size of the model's context,
which for GPT-3 was 2048 tokens, around 1500 words (citation needed, pricing page has this info).

\subsection{Bidirectional Encoder Representations from Transformers}
Bidirectional Encoder Representations from Transformers, henceforth referred to as BERT,
is a language model utilizing masking of words during training to achieve bidirectionality
in the model. \cite{devlin_bert_2019}

BERT uses a similar architecture as the original transformer. It uses the encoder with
multiple layers. The main difference is in the training methods. In the pre-training step,
two different training approaches are used: masked language model and next sentence prediction.
Masking allows the self-attention layer to take into account both sides of a given word, instead
of just one side, as in GPT which can only use the previous words as context. Using next sentence
prediction improves the model's understanding of the relationships between sentences. This is
useful for question answering and natural language inference type of tasks. \cite{devlin_bert_2019}

A masked language model (MLM) is achived by hiding a certain number of tokens, 12\% in the paper, and
asking the model to predict the hidden token. The model can then use both sides of the token as
context. Some of the tokens marked for masking are instead turned 
into random tokens or kept as is. This helps prevent a difference between pre-training and fine-tuning.
Other unidirectional methods predict the entire text during training while MLM only 
does so only for the masked tokens. \cite{devlin_bert_2019}

Next sentence prediction takes a pair of sentences, \texttt{A} and \texttt{B}, and requests the model
to predict whether \texttt{B} follows \texttt{A} or not. In the paper, 50\% of the time \texttt{B} 
was the next sentence and 50\% of the time it was not. \cite{devlin_bert_2019}

BERT also uses unsupervised pre-training and supervised fine-tuning as the general training setup \cite{devlin_bert_2019}.
The largest amount of work and time is spent pre-training the model on a large set of unstructured
text. This teaches the model general language understanding. Fine-tuning can then be used to 
train the model for specific natural language tasks leveraging the existing language knowledge 
from the pre-training stage. The pre-trained model is used to initialize a new model and 
then all the parameters are trained for a specific task. Fine-tuning is usually a more 
lightweight operation than pre-training. \cite{devlin_bert_2019}

\subsection{Text-to-Text Transfer Transformer}
\begin{figure*}
  \centering
  \begin{tikzpicture}[line width=0.3mm, align=center, font=\sffamily]
    % Nodes
    \matrix [matrix of nodes, row sep=2mm] (rightmat) {
      \node (translatein) [rounded corners=3pt, draw, fill=green!20] {"translate English to German: That is good."}; \\
      \node (colain)[rounded corners=3pt, draw, fill=red!20] {"cola sentence: The\\course is jumping well."}; \\
      \node (stsbin)[rounded corners=3pt, draw, fill=yellow!20] {"stsb sentence1: The rhino grazed\\on the grass. sentence2: A rhino\\is grazing in a field."}; \\
      \node (summaryin)[rounded corners=3pt, draw, fill=cyan!20] {"summarize: state authorities\\dispatched emergency crews tuesday to\\survey the damage after an onslaught\\of severe weather in mississippi..."}; \\
    };
    \node [right=of rightmat, rounded corners=3pt, draw, fill=gray!20] (t5) {\Huge T5};
    \matrix [matrix of nodes, right=of t5, row sep=2mm] {
      \node (translateout)[rounded corners=3pt, draw, fill=green!20] {"Das ist gut."}; \\
      \node (colaout)[rounded corners=3pt, draw, fill=red!20] {"not acceptable"}; \\
      \node (stsbout)[rounded corners=3pt, draw, fill=yellow!20]  {"3.8"}; \\
      \node (summaryout)[rounded corners=3pt, draw, fill=cyan!20] {"six people hospitalized after\\a storm in attala county."}; \\
    };
  
    % Lines
    \draw [-] (translatein.east) .. controls +(right:5mm) and +(left:5mm) .. (t5.west);
    \draw [-] (colain.east) .. controls +(right:5mm) and +(left:5mm) .. (t5.west);
    \draw [-] (stsbin.east) .. controls +(right:5mm) and +(left:5mm) .. (t5.west);
    \draw [-] (summaryin.east) .. controls +(right:5mm) and +(left:5mm) .. (t5.west);
  
    \draw [-] (translateout.west) .. controls +(left:5mm) and +(right:5mm) .. (t5.east);
    \draw [-] (colaout.west) .. controls +(left:5mm) and +(right:5mm) .. (t5.east);
    \draw [-] (stsbout.west) .. controls +(left:5mm) and +(right:5mm) .. (t5.east);
    \draw [-] (summaryout.west) .. controls +(left:5mm) and +(right:5mm) .. (t5.east);
  \end{tikzpicture}
  \caption{Illustrating the types of inputs and outputs. Recreated from \cite{raffel_exploring_2020}.}
\end{figure*}

Text-to-Text Transfer Transformer, abbreviated as T5, is a language model modelling each
task as text in, text out. It uses both the encoder and decoder layers of the transformer.
\cite{raffel_exploring_2020}

The main differentiating factor of T5 is the way it chooses to model tasks. Instead of 
tailoring each output to the specific task at hand, every task takes in a text prefix
describing the task and outputting the results as text. For example, for STS-B classification
task the model is trained to out one of a set of floating point numbers 
$x \in \{ 1+0.2k \mid k \in \{ 0, 1, ..., 20\} \}$ as a text. Many other models just 
output a number directly as a sliding scale from 1 to 5. 

\section{Applications}

\subsection{Toolformer}
Transformers are able to produce very natural text but as previously mentioned, they struggle with seemingly
simple tasks such as simple mathematical calculations and factual information gathering.
To get around of these limitations, transformers can be taught to use external tools
to offload these tasks to applications better suited to them. \cite{schick_toolformer_2023} 
This can be achieved using additional fine tuning, as is the case with Toolformer
\cite{schick_toolformer_2023}, or by using few-shot methods telling the model 
about the tools available to it, as in Toolformer Zero \cite{minosvasilias_markus_toolformer_2023}.

\subsection{Multimodal Models}
Large language models have proven themselves very capable at a variety of language related
tasks. Their usefulness quickly diminishes the further we stray from their training set.
Using multimodal models (check name) however, we can widen the area of tasks the model
can complete and even achieve positive transfer, benefitting from the variety of tasks
it is trained for \cite{driess_palm-e_2023}. 

PaLM-E is a multimodal language model utilizing a transformer based language model called
PaLM \cite{chowdhery_palm_2022,driess_palm-e_2023}. Multimodality refers to the fact that
the models accept types of input other than text. For example, PaLM-E accepts an input like
"\texttt{Q: What happened between <img\_1> and <img\_2>?}" \cite{driess_palm-e_2023} where
\texttt{<img\_1>} and \texttt{<img\_2>} refer to image embeddings. The E in PaLM-E refers to
the model being embodied. This means that the model is capable of performing tasks that
relate to real world objects. For example, the model can be used to plan high-level tasks
for a robot. \cite{driess_palm-e_2023}

\begin{figure}[h]
  \centering
  \includegraphics*[scale=0.1]{img/palm-e-transfer.png}
  \caption{
    PaLM-E displays positive transfer in learning. The performance is better for these three
    distinct robotics domains when trained together than when trained separately for the 
    specific tasks. \cite{driess_palm-e_2023}
  }
\end{figure}


\section{Problems}
\subsection{Biases}
The writers of \cite{brown_language_2020} did a review of gender, racial and religious
biases in the model they presented in the paper. Their conclusion was that there were
certain biases in place which matched the biases present in the training data, that being
the internet. 

Most highly educated and physical labor occupations had a male bias and in general, males
had a bias for being mentioned more often both in positive and negative contexts. Racial
sentiment depended on the size of the model, but in general, the sentiment for white,
black and middle eastern people was around neutral and for latinx, asian and indian it was
positive. For religion, violence was linked to islam at a greater rate than for other
major world religions.

\subsection{Hallucination}
Large languge models produce convincing text that can rival human written text \cite{brown_language_2020}.
It is possible to get these models to produce content which is 
not entirely factual or even completely nonsensical \cite{ji_survey_2023}. 

For example, when asking ChatGPT, which uses a slightly modified version of GPT-3 called 
GPT-3.5 [citation needed], to do a simple calculation, it answers confidently. 
With a prompt of "\texttt{what is 491*397}" we receive a response of 
"\texttt{The product of 491 and 397 is 195127.}". Unfortunately, using a calculator
leads to a different answer: 194927. Dangerously close, but not quite there.

Another example is asking the model about something they have not been trained on, for
example about future events. Prompting the model with 
"\texttt{What did Sauli Niinistö accomplish during his third term in office?}" returns
a logically perplexing response "\texttt{Sauli Niinistö is the current President of Finland, 
and he is currently serving his second consecutive term in office. 
His third term began on March 1, 2018, and it will end on February 28, 2024. ...}".
The response claims that Sauli Niinistö is currently serving his second term in office
which is correct as of 2023 and also in 2021 where the training data of GPT-3.5 ends 
[citation needed]. However, after this, the response claims that his third term began
in 2018, directly contradicting the previous statement. It is also important to mention
that there has been a two term limit since (insert year here) written into law 
[citation needed, finlex?]. Rest of the response after the first paragraph is removed.

While these limitations can be quite clear to professionals, many people without relevant
experience have no way of discerning factually correct answers from wrong ones. The confidence
by which the models answer can also mislead people into trusting the answers too much.



\printbibliography

\end{document}

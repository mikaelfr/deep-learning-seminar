
\documentclass[a4paper,11pt]{article}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{varwidth,xcolor}
\usepackage{graphicx}
\usepackage{amssymb}


\newcommand{\veps}{\varepsilon}
\newcommand{\ee}{{\rm e}\hspace{1pt}}


\begin{document}
\pagestyle{empty}


  \begin{center}
{\large {\bf 
Review on a report \\
``Generative Adversarial Networks'' \\
by Victor Manuel Yeom Song} \\

\bigskip

Written by: 

Mikael Fredriksson
 } 
\end{center}

The text is very clearly written. On top of excellent and succint explanations and 
descriptions, graphs and figures are used tastefully when necessary. The paper is 
well balanced between equations, figures and text.

I liked that the concise and exact math definitions were accompanied with more
loose intuitive explanations. This helped me get a rough understanding first and 
then diving into the math was much easier than if there would've been only the
pure math explanations.

As a practical, hands-on guy, I usually prefer to see a bit more examples. In this case,
though, I feel like the distribution of space given to different topics was great.
Explaining the way GANs work and the improvements over the basic implementation takes 
some space and is more important in this case. The organization of the different
topics was very logical, starting from the baseline, explaining the improvements
in chronological order and then talking about examples and conclusions.

I would've hoped to see Jensen-Shannon Divergence explained some more.
The same could probably also be said for Kantorovich-Rubinstein duality but that is
not as important, since it's just a way of getting an equivalent representation
for the loss function which is more easily optimized. Jensen-Shannon Divergence, on
the other hand, was directly an inseparable part of the minimized function. Then
again, the point of the paper is to be explain GANs and not teach math.

The conclusion is short, simple and to the point. It recaps the topics discussed and
offers quite generic commentary on the popularity of GANs. It works well as a conclusion
but if you want to work on it a bit more, maybe adding something with your own 
perspective could work. At least that was something I would've liked seeing.

Overall, the text was a pleasure to read. I feel like it gave me a good overview
of the inner workings of GANs and serves a good foundation to continue reading further.


As a final note, I noticed a slight typo in the beginning:

\begin{itemize}
	
	\item First sentence in the abstract, machine learning is misspelled missing an n.

\end{itemize}


\end{document}
